# üî¨ Gu√≠a de Evaluaci√≥n Comparativa Multi-Dataset

## Descripci√≥n

Este sistema permite evaluar tu modelo de predicci√≥n de stock con **m√∫ltiples datasets** de forma automatizada. Ejecuta todo el pipeline completo (preprocesamiento ‚Üí entrenamiento ‚Üí evaluaci√≥n) para cada dataset y genera comparaciones visuales.

## üéØ ¬øQu√© hace este sistema?

Para cada dataset disponible:
1. **Carga** los datos
2. **Preprocesa** (feature engineering, codificaci√≥n, normalizaci√≥n)
3. **Crea secuencias temporales** (con ventana de 7 d√≠as)
4. **Entrena** un modelo GRU
5. **Eval√∫a** el modelo y calcula m√©tricas
6. **Genera visualizaciones** completas
7. **Compara** resultados entre datasets

## üìã Estructura del Notebook

### Secci√≥n 1: Clase Principal de Evaluaci√≥n
- `ModeloStockEvaluador`: Encapsula todo el pipeline para un dataset

### Secci√≥n 2: Sistema Multi-Dataset
- `EvaluadorMultiDataset`: Ejecuta evaluaci√≥n en m√∫ltiples datasets

### Secci√≥n 3: Ejecutar Evaluaci√≥n Completa
- Ejecuta el pipeline para **TODOS** los datasets disponibles

### Secci√≥n 4: Generar Reporte Completo
- Genera visualizaciones comparativas
- Crea tablas resumen
- Identifica el mejor dataset

### Secci√≥n 5: An√°lisis Individual
- An√°lisis detallado de un dataset espec√≠fico

### Secci√≥n 6: Exportar Resultados
- Guarda predicciones, m√©tricas, modelos y historia

### Secci√≥n 7: Prueba R√°pida
- Eval√∫a solo un dataset para pruebas

## üöÄ Uso R√°pido

### Opci√≥n 1: Evaluar TODOS los datasets (Recomendado)

```python
# En la Secci√≥n 3, ejecutar las celdas en orden:
# 1. Configuraci√≥n
# 2. Iniciar evaluaci√≥n
# 3. Generar reporte completo
```

**Tiempo estimado:** ~2-5 minutos por dataset

### Opci√≥n 2: Prueba R√°pida con UN dataset

```python
# En la Secci√≥n 7, configurar:
DATASET_PRUEBA = './data/subset_10_productos.csv'
# Y ejecutar
```

## üìä Visualizaciones Generadas

### Por cada dataset:
1. **Curvas de entrenamiento** (Loss y MAE)
2. **Scatter plot** (Predicciones vs Reales)
3. **Histograma de errores**
4. **Boxplot de errores**
5. **Panel de m√©tricas** detallado

### Comparativas entre datasets:
1. **MAE vs Tama√±o** del dataset
2. **RMSE vs Tama√±o**
3. **Error relativo vs Tama√±o**
4. **Tiempo de entrenamiento vs Tama√±o**
5. **√âpocas entrenadas** por dataset
6. **Tabla comparativa** completa

## üìà M√©tricas Calculadas

Para cada dataset se calculan:

### M√©tricas Normalizadas (0-1):
- **RMSE** (escalado)
- **MAE** (escalado)

### M√©tricas Reales (unidades de stock):
- **RMSE** (unidades)
- **MAE** (unidades)
- **Error relativo** (%)

### Informaci√≥n del Dataset:
- Total de productos √∫nicos
- Total de filas
- Secuencias de train/validaci√≥n
- Rango de stock (min/max)

### Tiempos:
- Tiempo de carga
- Tiempo de preprocesamiento
- Tiempo de creaci√≥n de secuencias
- Tiempo de entrenamiento
- Tiempo de evaluaci√≥n
- **Tiempo total**

## üéì Interpretaci√≥n de Resultados

### MAE (Mean Absolute Error)
- **Bajo es mejor**
- Representa el error promedio en unidades de stock
- Ejemplo: MAE = 25.5 significa que las predicciones se desv√≠an ¬±25.5 unidades en promedio

### RMSE (Root Mean Squared Error)
- **Bajo es mejor**
- Penaliza m√°s los errores grandes
- Siempre mayor o igual que MAE

### Error Relativo (%)
- **Bajo es mejor**
- Error como porcentaje del rango total de stock
- Facilita comparaci√≥n entre datasets con diferentes escalas
- Ejemplo: 5% es excelente, 15% es aceptable, >25% es mejorable

### √âpocas Entrenadas
- Si es cercano al m√°ximo (100), el modelo quiz√°s necesite m√°s tiempo
- Si es bajo (<20), el modelo converge r√°pido (puede ser overfitting o underfitting)
- Ideal: 30-70 √©pocas con early stopping

## üí° Casos de Uso

### 1. Comparar Tama√±os de Dataset
**Pregunta:** ¬øCu√°ntos datos necesito para un buen modelo?

```python
# Ejecutar Secci√≥n 3 y 4
# Observar gr√°ficos "MAE vs Tama√±o del Dataset"
# Identificar punto de rendimientos decrecientes
```

### 2. Optimizar Tiempo de Entrenamiento
**Pregunta:** ¬øCu√°l es el mejor balance entre precisi√≥n y tiempo?

```python
# Revisar gr√°fico "Tiempo de Entrenamiento vs Tama√±o"
# Comparar con MAE para encontrar sweet spot
```

### 3. Validar Modelo en Producci√≥n
**Pregunta:** ¬øQu√© dataset usar para deployment?

```python
# Usar evaluador_multi.mejor_dataset(metrica='mae_real')
# Considerar tambi√©n tiempo de inferencia
```

### 4. Debugging de Modelo
**Pregunta:** ¬øPor qu√© el modelo no mejora?

```python
# Secci√≥n 5: An√°lisis individual
# Revisar histograma de errores
# Ver si hay patrones en scatter plot
```

## üìÅ Archivos Generados

Despu√©s de ejecutar el sistema, se generar√°n:

```
resultados_comparacion.csv          # Tabla comparativa completa
resultados_evaluacion/
    ‚îú‚îÄ‚îÄ predicciones_subset_XXX.csv  # Predicciones de cada dataset
    ‚îú‚îÄ‚îÄ metricas_subset_XXX.json     # M√©tricas detalladas
    ‚îú‚îÄ‚îÄ modelo_subset_XXX.keras      # Modelo entrenado
    ‚îî‚îÄ‚îÄ historia_subset_XXX.csv      # Historia de entrenamiento
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Ajustar Par√°metros de Entrenamiento

En la **Secci√≥n 3**:

```python
EPOCHS = 100          # M√°s √©pocas = m√°s tiempo, posiblemente mejor modelo
BATCH_SIZE = 64       # Mayor = m√°s r√°pido pero m√°s memoria
VERBOSE = 0           # 1 para ver progreso
```

### Cambiar Arquitectura del Modelo

Modificar en `ModeloStockEvaluador.entrenar_modelo()`:

```python
# Cambiar unidades GRU
self.model.add(GRU(units=128, ...))  # Era 64

# A√±adir m√°s capas
self.model.add(GRU(units=64, return_sequences=True, ...))
self.model.add(GRU(units=32, ...))

# Ajustar Dropout
self.model.add(Dropout(0.3, ...))  # Era 0.2
```

### Cambiar Divisi√≥n Train/Val

```python
evaluador = ModeloStockEvaluador(
    dataset_path='...',
    nombre_experimento='...',
    split_percentage=0.85  # Era 0.8 (80/20)
)
```

## üêõ Soluci√≥n de Problemas

### "No se encontraron datasets"
- Verifica que los archivos est√©n en `./data/`
- Verifica el patr√≥n de b√∫squeda: `subset_*.csv`

### "Out of memory"
- Reduce `BATCH_SIZE`
- Procesa menos datasets a la vez
- Usa datasets m√°s peque√±os primero

### "El entrenamiento es muy lento"
- Reduce `EPOCHS`
- Aumenta `BATCH_SIZE` (si hay memoria)
- Usa GPU si est√° disponible

### "Las m√©tricas no mejoran"
- Revisa la distribuci√≥n de errores (histograma)
- Verifica que hay suficientes datos de entrenamiento
- Considera ajustar la arquitectura del modelo
- Revisa el preprocesamiento de datos

## üìû Flujo de Trabajo Recomendado

1. **Prueba r√°pida** (Secci√≥n 7)
   - Ejecuta con el dataset m√°s peque√±o (10 productos)
   - Verifica que todo funciona
   - Tiempo: ~30 segundos

2. **Evaluaci√≥n parcial**
   - Selecciona 2-3 datasets de diferentes tama√±os
   - Ejecuta Secci√≥n 3 con solo esos datasets
   - Tiempo: ~5-10 minutos

3. **Evaluaci√≥n completa**
   - Ejecuta con todos los datasets
   - Genera reporte completo
   - Tiempo: ~15-30 minutos

4. **An√°lisis profundo**
   - Revisa el mejor dataset (Secci√≥n 5)
   - Exporta resultados (Secci√≥n 6)
   - Documenta hallazgos

## üéØ Ejemplo Completo

```python
# 1. Crear evaluador multi-dataset
evaluador_multi = EvaluadorMultiDataset(datasets_dir='./data')

# 2. Detectar datasets
datasets = evaluador_multi.detectar_datasets(patron='subset_*.csv')

# 3. Evaluar todos
evaluador_multi.evaluar_todos(
    datasets=datasets,
    epochs=100,
    batch_size=64,
    verbose=0
)

# 4. Generar reporte
df_resultados = evaluador_multi.generar_reporte_completo(guardar=True)

# 5. Encontrar mejor
mejor = evaluador_multi.mejor_dataset(metrica='mae_real')

# 6. Analizar mejor dataset
eval_mejor = evaluador_multi.evaluadores[mejor['nombre']]
eval_mejor.generar_visualizaciones()
```

## üìö Recursos Adicionales

- **Pipeline original**: Ver celdas iniciales del notebook para entender cada paso
- **Generador de datasets**: Ver `generador_subsets.py` para crear nuevos subsets
- **Documentaci√≥n datasets**: Ver `README_SUBSETS.md`

---

**¬°Feliz an√°lisis de modelos! üöÄüìä**
